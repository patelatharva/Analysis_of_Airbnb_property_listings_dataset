{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from scikit-optimize) (1.2.1)\n",
      "Requirement already satisfied: numpy in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from scikit-optimize) (1.16.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from scikit-optimize) (0.20.3)\n",
      "Requirement already satisfied: hyperopt in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (1.16.2)\n",
      "Requirement already satisfied: six in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (1.12.0)\n",
      "Requirement already satisfied: networkx in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (2.2)\n",
      "Requirement already satisfied: pymongo in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (3.9.0)\n",
      "Requirement already satisfied: tqdm in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (4.31.1)\n",
      "Requirement already satisfied: future in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (0.17.1)\n",
      "Requirement already satisfied: scipy in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from hyperopt) (1.2.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages (from networkx->hyperopt) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "import math\n",
    "from sklearn.base import clone\n",
    "!pip install scikit-optimize\n",
    "from skopt import gp_minimize\n",
    "!pip install hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar_dfs = []\n",
    "# for file_name in glob.glob('./data/amsterdam/calendar*.gz'):\n",
    "#     calendar_dfs.append(pd.read_csv(file_name, compression='gzip'))\n",
    "# calendar = pd.concat(calendar_dfs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_dfs = []\n",
    "# for file_name in glob.glob('./data/amsterdam/reviews*.gz'):\n",
    "#     reviews_dfs.append(pd.read_csv(file_name, compression='gzip'))\n",
    "# reviews = pd.concat(reviews_dfs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listings_dfs = []\n",
    "# for file_name in glob.glob('./data/amsterdam/listings*.gz'):\n",
    "#     listings_dfs.append(pd.read_csv(file_name, compression='gzip', dtype=\"str\"))\n",
    "# listings = pd.concat(listings_dfs, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(\"./data/boston/calendar.csv\")\n",
    "reviews = pd.read_csv(\"./data/boston/reviews.csv\")\n",
    "listings = pd.read_csv(\"./data/boston/listings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming 'price' from string to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    250.0\n",
       "1     65.0\n",
       "2     65.0\n",
       "3     75.0\n",
       "4     79.0\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Converts price in the string format like \"$1,125.00\" into numeric value 1125.00\n",
    "    INPUT:\n",
    "    - string price in string format\n",
    "    OUTPUT:\n",
    "    - float value corresponding to the price or None if the input is not parseable to float\n",
    "\"\"\"\n",
    "def str_to_num (string):\n",
    "    if string is not None:\n",
    "        if type(string) is str and string.startswith('$'):\n",
    "            return float(string.replace('$', '').replace(',', ''))\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "listings_cleaned = pd.concat([listings.drop('price', axis=1), listings[\"price\"].apply(str_to_num)], axis=1)\n",
    "listings_cleaned['price'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   NaN\n",
       "1   NaN\n",
       "2   NaN\n",
       "3   NaN\n",
       "4   NaN\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_cleaned = pd.concat([calendar.drop('price', axis=1), calendar[\"price\"].apply(str_to_num)], axis=1)\n",
    "calendar_cleaned['price'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting 'avilable' field from string to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: available, dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_cleaned['available'] = calendar_cleaned['available'] == 't'\n",
    "calendar_cleaned['available'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting 'date' from string to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_cleaned[\"date\"] = pd.to_datetime(calendar_cleaned[\"date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting location field from numeric to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_cleaned[\"location_categorical\"] = listings_cleaned.apply(lambda row: str((format(row.latitude, '.2f'), format(row.longitude, '.2f'))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ('42.28', '-71.13')\n",
       "1    ('42.29', '-71.13')\n",
       "2    ('42.29', '-71.14')\n",
       "3    ('42.28', '-71.12')\n",
       "4    ('42.28', '-71.14')\n",
       "Name: location_categorical, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_cleaned.location_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting 'amenities' field containing list into separate categorical columns for each amenity in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TV',\n",
       " 'Wireless Internet',\n",
       " 'Kitchen',\n",
       " 'Free Parking on Premises',\n",
       " 'Pets live on this property',\n",
       " 'Dog(s)',\n",
       " 'Heating',\n",
       " 'Family/Kid Friendly',\n",
       " 'Washer',\n",
       " 'Dryer',\n",
       " 'Smoke Detector',\n",
       " 'Fire Extinguisher',\n",
       " 'Essentials',\n",
       " 'Shampoo',\n",
       " 'Laptop Friendly Workspace']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda amenity : amenity.replace(\"\\\"\",\"\").replace(\"{\",\"\").replace(\"}\", \"\"), listings_cleaned.amenities.iloc[0].split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Separates the string value of `amenities` attribute in the row, into a list of individual amenities.\n",
    "    \n",
    "    INPUT:\n",
    "    - row : from dataset having amenities attribute\n",
    "    OUTPUT:    \n",
    "    - list of amenities derived from the value of `amenities` attribute in row\n",
    "\"\"\"\n",
    "def separate_amenities(row):\n",
    "    amenities = row.amenities\n",
    "    list_to_return = []\n",
    "    if (amenities is not None and type(amenities) == str):            \n",
    "        list_to_return = list(map(lambda amenity : amenity.replace(\"\\\"\",\"\").replace(\"{\",\"\").replace(\"}\", \"\"), amenities.split(\",\")))\n",
    "    if '' in list_to_return:\n",
    "        list_to_return.remove('')\n",
    "    return list_to_return\n",
    "listings_cleaned[\"amenities_list\"] = listings_cleaned.apply(separate_amenities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TV', 'Wireless Internet', 'Kitchen', 'Free Parking on Premises',\n",
       "       'Pets live on this property', 'Dog(s)', 'Heating',\n",
       "       'Family/Kid Friendly', 'Washer', 'Dryer', 'Smoke Detector',\n",
       "       'Fire Extinguisher', 'Essentials', 'Shampoo',\n",
       "       'Laptop Friendly Workspace', 'Internet', 'Air Conditioning',\n",
       "       'Pets Allowed', 'Carbon Monoxide Detector', 'Lock on Bedroom Door',\n",
       "       'Hangers', 'Hair Dryer', 'Iron', 'Cable TV', 'First Aid Kit',\n",
       "       'Safety Card', 'translation missing: en.hosting_amenity_49',\n",
       "       'translation missing: en.hosting_amenity_50', 'Gym', 'Breakfast',\n",
       "       'Indoor Fireplace', 'Cat(s)', '24-Hour Check-in', 'Hot Tub',\n",
       "       'Buzzer/Wireless Intercom', 'Other pet(s)', 'Washer / Dryer',\n",
       "       'Smoking Allowed', 'Suitable for Events', 'Wheelchair Accessible',\n",
       "       'Elevator in Building', 'Pool', 'Doorman',\n",
       "       'Paid Parking Off Premises', 'Free Parking on Street'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_amenities = listings_cleaned['amenities_list'].apply(pd.Series).stack().unique()\n",
    "possible_amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Assigns new boolean attribute to the row based on the presence of that amenity in the list.\n",
    "    Returns updated row with additional attributes corresponding to the amenities added to it.\n",
    "    \n",
    "    INPUT:\n",
    "    - row containing attributes related to the property, including `amenities_list`\n",
    "    OUTPUT:\n",
    "    - row containing newly added boolean attributes indicating the presence of each possible type of amenity in the property\n",
    "\"\"\"\n",
    "def add_amenities_columns (row):\n",
    "    amenities = set(row.amenities_list)\n",
    "    for possible_amenity in possible_amenities:\n",
    "        row[\"amenity_\" + possible_amenity] = possible_amenity in amenities\n",
    "    return row\n",
    "\n",
    "listings_cleaned = listings_cleaned.apply(add_amenities_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the activation date for each property listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3353</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>True</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3353</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3353</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>True</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3353</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5506</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-10-10</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-28</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-19</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-15</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5506</td>\n",
       "      <td>2016-09-08</td>\n",
       "      <td>True</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6695</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>True</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6695</td>\n",
       "      <td>2016-10-24</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6695</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>True</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    listing_id       date  available  price\n",
       "0         3353 2017-09-05       True   36.0\n",
       "1         3353 2016-12-30      False    NaN\n",
       "2         3353 2017-08-18       True   36.0\n",
       "3         3353 2016-10-12      False    NaN\n",
       "4         5506 2017-09-05       True  145.0\n",
       "5         5506 2016-10-10      False    NaN\n",
       "6         5506 2016-10-03       True  145.0\n",
       "7         5506 2016-09-30      False    NaN\n",
       "8         5506 2016-09-28       True  145.0\n",
       "9         5506 2016-09-25      False    NaN\n",
       "10        5506 2016-09-22       True  145.0\n",
       "11        5506 2016-09-21      False    NaN\n",
       "12        5506 2016-09-19       True  145.0\n",
       "13        5506 2016-09-18      False    NaN\n",
       "14        5506 2016-09-15       True  145.0\n",
       "15        5506 2016-09-12      False    NaN\n",
       "16        5506 2016-09-08       True  145.0\n",
       "17        6695 2017-09-05       True  195.0\n",
       "18        6695 2016-10-24      False    NaN\n",
       "19        6695 2016-10-19       True  195.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = calendar_cleaned\n",
    "df = df.groupby('listing_id', group_keys=False)\\\n",
    "    .apply(lambda x: x[x.available.ne(x.available.shift())])\\\n",
    "    .reset_index(drop=True)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_activation_dates = df[df.available == True].groupby(\"listing_id\")[[\"listing_id\",\"date\"]].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>530983</th>\n",
       "      <td>530983</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815639</th>\n",
       "      <td>815639</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12726343</th>\n",
       "      <td>12726343</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776391</th>\n",
       "      <td>2776391</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10524612</th>\n",
       "      <td>10524612</td>\n",
       "      <td>2016-09-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            listing_id       date\n",
       "listing_id                       \n",
       "530983          530983 2016-09-06\n",
       "815639          815639 2016-09-06\n",
       "12726343      12726343 2016-09-06\n",
       "2776391        2776391 2016-09-06\n",
       "10524612      10524612 2016-09-06"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listing_activation_dates.sort_values(by=[\"date\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only retaining entries in calendar for each property after its activation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_by_listing_groups = calendar_cleaned.groupby([\"listing_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-03</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12147973</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>True</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>3075044</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>True</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-12</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-10</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>6976</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>True</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-10</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-07</td>\n",
       "      <td>True</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>1436513</td>\n",
       "      <td>2017-05-06</td>\n",
       "      <td>True</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-18</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>7651065</td>\n",
       "      <td>2017-06-17</td>\n",
       "      <td>True</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-25</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-24</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-22</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>12386020</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306700</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306701</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306702</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306703</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306704</th>\n",
       "      <td>14852179</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307065</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-18</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307066</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-17</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307067</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307068</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307069</th>\n",
       "      <td>8373729</td>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>True</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307430</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307431</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307432</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307433</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307434</th>\n",
       "      <td>14844274</td>\n",
       "      <td>2017-06-18</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307795</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307796</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307797</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-03</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307798</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307799</th>\n",
       "      <td>14585486</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308160</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308161</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308162</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-03</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308163</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-02</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308164</th>\n",
       "      <td>14603878</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>True</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308525</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308526</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308527</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308528</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-18</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308529</th>\n",
       "      <td>14504422</td>\n",
       "      <td>2017-06-17</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17925 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         listing_id       date  available  price\n",
       "0          12147973 2017-09-05      False    NaN\n",
       "1          12147973 2017-09-04      False    NaN\n",
       "2          12147973 2017-09-03      False    NaN\n",
       "3          12147973 2017-09-02      False    NaN\n",
       "4          12147973 2017-09-01      False    NaN\n",
       "365         3075044 2017-08-22       True   65.0\n",
       "366         3075044 2017-08-21       True   65.0\n",
       "367         3075044 2017-08-20       True   65.0\n",
       "368         3075044 2017-08-19       True   75.0\n",
       "369         3075044 2017-08-18       True   75.0\n",
       "730            6976 2017-05-12       True   65.0\n",
       "731            6976 2017-05-11       True   65.0\n",
       "732            6976 2017-05-10       True   65.0\n",
       "733            6976 2017-05-09       True   65.0\n",
       "734            6976 2017-05-08       True   65.0\n",
       "1095        1436513 2017-05-10      False    NaN\n",
       "1096        1436513 2017-05-09      False    NaN\n",
       "1097        1436513 2017-05-08      False    NaN\n",
       "1098        1436513 2017-05-07       True   75.0\n",
       "1099        1436513 2017-05-06       True   75.0\n",
       "1460        7651065 2017-06-21       True   79.0\n",
       "1461        7651065 2017-06-20       True   79.0\n",
       "1462        7651065 2017-06-19       True   79.0\n",
       "1463        7651065 2017-06-18       True   79.0\n",
       "1464        7651065 2017-06-17       True   79.0\n",
       "1825       12386020 2017-04-25      False    NaN\n",
       "1826       12386020 2017-04-24      False    NaN\n",
       "1827       12386020 2017-04-23      False    NaN\n",
       "1828       12386020 2017-04-22      False    NaN\n",
       "1829       12386020 2017-04-21      False    NaN\n",
       "...             ...        ...        ...    ...\n",
       "1306700    14852179 2017-08-20      False    NaN\n",
       "1306701    14852179 2017-08-19      False    NaN\n",
       "1306702    14852179 2017-08-18      False    NaN\n",
       "1306703    14852179 2017-08-17      False    NaN\n",
       "1306704    14852179 2017-08-16      False    NaN\n",
       "1307065     8373729 2017-05-18       True   69.0\n",
       "1307066     8373729 2017-05-17       True   69.0\n",
       "1307067     8373729 2017-05-16       True   69.0\n",
       "1307068     8373729 2017-05-15       True   69.0\n",
       "1307069     8373729 2017-05-14       True   69.0\n",
       "1307430    14844274 2017-06-22       True  150.0\n",
       "1307431    14844274 2017-06-21       True  150.0\n",
       "1307432    14844274 2017-06-20       True  150.0\n",
       "1307433    14844274 2017-06-19       True  150.0\n",
       "1307434    14844274 2017-06-18       True  150.0\n",
       "1307795    14585486 2017-09-05      False    NaN\n",
       "1307796    14585486 2017-09-04      False    NaN\n",
       "1307797    14585486 2017-09-03      False    NaN\n",
       "1307798    14585486 2017-09-02      False    NaN\n",
       "1307799    14585486 2017-09-01      False    NaN\n",
       "1308160    14603878 2017-09-05       True   59.0\n",
       "1308161    14603878 2017-09-04       True   59.0\n",
       "1308162    14603878 2017-09-03       True   59.0\n",
       "1308163    14603878 2017-09-02       True   59.0\n",
       "1308164    14603878 2017-09-01       True   59.0\n",
       "1308525    14504422 2017-06-21      False    NaN\n",
       "1308526    14504422 2017-06-20      False    NaN\n",
       "1308527    14504422 2017-06-19      False    NaN\n",
       "1308528    14504422 2017-06-18      False    NaN\n",
       "1308529    14504422 2017-06-17      False    NaN\n",
       "\n",
       "[17925 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_by_listing_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_entries_after_activation_date(g):\n",
    "\n",
    "    listing_id = g.name\n",
    "    activation_date_df = listing_activation_dates.query(\"listing_id == @listing_id\")[\"date\"]\n",
    "    if activation_date_df.shape[0] > 0:        \n",
    "        activation_date = activation_date_df.iloc[0]\n",
    "#         print(\"activatation_date: \" + str(activation_date))\n",
    "#         print(g[\"date\"].dtype)\n",
    "#         print(type(activation_date))\n",
    "        return g[g[\"date\"] <= activation_date]\n",
    "    \n",
    "    \n",
    "cal_after_activation_dates =cal_by_listing_groups.apply(select_entries_after_activation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and add necessary variables to be used for predicting occupancy of property at given time of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars_from_listings = [\"neighbourhood_cleansed\", \n",
    "#                 \"neighbourhood_group_cleansed\",\n",
    "                \"city\",\n",
    "                \"state\",\n",
    "                \"zipcode\",\n",
    "                \"market\",\n",
    "                \"location_categorical\",\n",
    "                \"property_type\",\n",
    "                \"room_type\",\n",
    "                \"accommodates\",\n",
    "                \"bathrooms\",\n",
    "                \"bedrooms\",\n",
    "                \"beds\",\n",
    "                \"bed_type\",\n",
    "                \"square_feet\",\n",
    "                \"guests_included\",\n",
    "                \"minimum_nights\",\n",
    "                \"maximum_nights\",\n",
    "                \"review_scores_rating\",\n",
    "                \"review_scores_accuracy\",\n",
    "                  \"review_scores_cleanliness\",\n",
    "                  \"review_scores_checkin\",\n",
    "                  \"review_scores_communication\",\n",
    "                  \"review_scores_location\",\n",
    "                  \"review_scores_value\",\n",
    "#                   \"jurisdiction_names\",\n",
    "                  \"cancellation_policy\",\n",
    "                  \"reviews_per_month\",\n",
    "                  \"number_of_reviews\"\n",
    "       ]\n",
    "amenity_variables = list(map(lambda amenity : \"amenity_\" + amenity, possible_amenities))\n",
    "input_vars_from_listings.extend(amenity_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_cleaned[\"month\"] = calendar_cleaned['date'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_cleaned[\"day_of_week\"] = calendar_cleaned['date'].dt.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_cleaned[\"week_of_month\"] = np.ceil(calendar_cleaned['date'].dt.day/7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 4, 3, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_cleaned[\"week_of_month\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(listings_cleaned, calendar_cleaned, left_on=\"id\", right_on=\"listing_id\", how=\"inner\", suffixes=(\"_listings\", \"_calendar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby([\"listing_id\"]).apply(lambda x: x.sort_values(by=[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.loc[:, \"price_calendar\"] = g[\"price_calendar\"].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = g.drop(columns=[\"listing_id\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars_from_calendar = [\"month\", \"day_of_week\", \"week_of_month\", \"price_calendar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_vars = input_vars_from_listings + input_vars_from_calendar;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"available\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[all_input_vars + [\"listing_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"available\"] + [\"listing_id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values in numeric columns with mean of the corresponding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = X.select_dtypes(exclude=['object']).copy().columns\n",
    "def fill_mean (col):\n",
    "    return col.fillna(col.mean())\n",
    "X.loc[X.index, num_vars] = X[num_vars].apply(fill_mean, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical variables into numeric variables with separate column for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = X.select_dtypes(include=['object']).copy().columns\n",
    "for var in  cat_vars:\n",
    "    # for each cat add dummy var, drop original column\n",
    "    X = pd.concat([X.drop(var, axis=1), pd.get_dummies(X[var], prefix=var, prefix_sep='_', drop_first=False, dummy_na=True)], axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training and test sets while making sure that no property listing is common between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_listing_ids = X[\"listing_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "listing_ids_train = np.random.choice(np.array(unique_listing_ids), size= int(0.70 * len(unique_listing_ids)), replace=False)\n",
    "listing_ids_test = [l for l in unique_listing_ids if l not in listing_ids_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.query(\"listing_id in @listing_ids_train\")\n",
    "y_train = y.query(\"listing_id in @listing_ids_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X.query(\"listing_id in @listing_ids_test\").drop(columns=[\"listing_id\"])\n",
    "y_test = y.query(\"listing_id in @listing_ids_test\").drop(columns=[\"listing_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.510366\n",
       "True     0.489634\n",
       "Name: available, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[\"available\"].value_counts()/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the ratio of samples classified for output True and False, the data looks to be balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing cross validation splits to into training and test sets while making sure that no property listing is common between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGridSearchCV(object):\n",
    "    \n",
    "    def __init__(self, estimator, param_grid, cv=3):\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.best_score = -math.inf\n",
    "        self.best_estimator = None\n",
    "        self.best_estimator_scores_mean = -math.inf\n",
    "        self.best_estimator_scores_stdev = -math.inf\n",
    "        self.best_params= None\n",
    "        self.cv = cv       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for param_point in self.get_param_grid_points():\n",
    "            def splits(array, k):\n",
    "                splits = []\n",
    "                \n",
    "                for i in range(k):\n",
    "                    split_length = math.ceil(len(array)/k)\n",
    "                    split = []\n",
    "                    for j in range(i * split_length, (i+1) * split_length):\n",
    "                        if j < len(array):\n",
    "                            split.append(array[j])\n",
    "                        \n",
    "                    splits.append(split)\n",
    "                cvs = []\n",
    "                for i, split in enumerate(splits):\n",
    "                    train = []\n",
    "                    for j, other_split in enumerate(splits):\n",
    "                        if i != j:\n",
    "                            train += other_split\n",
    "                    test = [] + split\n",
    "                    cvs.append((train, test))\n",
    "                return cvs\n",
    "                    \n",
    "            scores_for_point = []\n",
    "            estimator = clone(self.estimator)\n",
    "            estimator.set_params(**param_point)\n",
    "            for l_train, l_test in splits(list(X[\"listing_id\"].unique()), k=self.cv):                \n",
    "                X_train = X.query(\"listing_id in @l_train\").drop(columns=[\"listing_id\"])\n",
    "                X_test = X.query(\"listing_id in @l_test\").drop(columns=[\"listing_id\"])\n",
    "                y_train = y.query(\"listing_id in @l_train\").drop(columns=[\"listing_id\"])\n",
    "                y_test = y.query(\"listing_id in @l_test\").drop(columns=[\"listing_id\"])                \n",
    "                estimator.fit(X_train, y_train.values.ravel())\n",
    "                y_preds = estimator.predict(X_test)\n",
    "                f1_score_1 = f1_score(y_test, y_preds, pos_label=True)\n",
    "                f1_score_0 = f1_score(y_test, y_preds, pos_label=False)\n",
    "                score = min([f1_score_1, f1_score_0])\n",
    "                scores_for_point.append(score)\n",
    "                \n",
    "            mean_score = np.mean(scores_for_point)\n",
    "            if mean_score > self.best_estimator_scores_mean:\n",
    "                self.best_estimator_scores_mean = mean_score\n",
    "                self.best_estimator_scores_stdev = np.std(scores_for_point)\n",
    "                self.best_estimator = estimator\n",
    "                self.best_params = self.best_estimator.get_params()\n",
    "                self.best_score = self.best_estimator_scores_mean\n",
    "   \n",
    "            \n",
    "    def get_param_grid_points(self):\n",
    "        points = [{}]\n",
    "        for param, values in self.param_grid.items():\n",
    "            new_points = []\n",
    "            for point in points:\n",
    "                for value in values:                    \n",
    "                    new_point = point.copy()\n",
    "                    new_point[param]= value\n",
    "                    new_points.append(new_point)\n",
    "            points = new_points\n",
    "        return points\n",
    "    \n",
    "    def print_best(self):\n",
    "        print(\"Best score:\")\n",
    "        print(self.best_score)\n",
    "        print(\"Best params:\")\n",
    "        print(self.best_params)\n",
    "        print(\"Best estimator scores mean:\")\n",
    "        print(self.best_estimator_scores_mean)\n",
    "        print(\"Best estimator scores stdev:\")\n",
    "        print(self.best_estimator_scores_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cross_val_score(estimator, X, y):\n",
    "    def splits(array, k):\n",
    "        splits = []\n",
    "\n",
    "        for i in range(k):\n",
    "            split_length = math.ceil(len(array)/k)\n",
    "            split = []\n",
    "            for j in range(i * split_length, (i+1) * split_length):\n",
    "                if j < len(array):\n",
    "                    split.append(array[j])\n",
    "\n",
    "            splits.append(split)\n",
    "        cvs = []\n",
    "        for i, split in enumerate(splits):\n",
    "            train = []\n",
    "            for j, other_split in enumerate(splits):\n",
    "                if i != j:\n",
    "                    train += other_split\n",
    "            test = [] + split\n",
    "            cvs.append((train, test))\n",
    "        return cvs\n",
    "\n",
    "    scores_for_point = []\n",
    "    for l_train, l_test in splits(list(X[\"listing_id\"].unique()), k=3):                \n",
    "        X_train = X.query(\"listing_id in @l_train\").drop(columns=[\"listing_id\"])\n",
    "        X_test = X.query(\"listing_id in @l_test\").drop(columns=[\"listing_id\"])\n",
    "        y_train = y.query(\"listing_id in @l_train\").drop(columns=[\"listing_id\"])\n",
    "        y_test = y.query(\"listing_id in @l_test\").drop(columns=[\"listing_id\"])                \n",
    "        estimator.fit(X_train, y_train.values.ravel())\n",
    "        y_preds = estimator.predict(X_test)\n",
    "        f1_score_1 = f1_score(y_test, y_preds, pos_label=True)\n",
    "        f1_score_0 = f1_score(y_test, y_preds, pos_label=False)\n",
    "        score = min([f1_score_1, f1_score_0])\n",
    "        scores_for_point.append(score)\n",
    "\n",
    "    mean_score = np.mean(scores_for_point)\n",
    "    return mean_score\n",
    "\n",
    "import decimal\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    start = decimal.Decimal(start)\n",
    "    stop = decimal.Decimal(stop)\n",
    "    step = decimal.Decimal(step)\n",
    "    while (start < stop):\n",
    "        yield float(start)\n",
    "        start += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Bayesian optimization technique for choosing hyper parameters for each classifier. \n",
    "\n",
    "Please note that I am choosing the number of points explored in the parameter space to be 100 which can be considered as too small. However, keeping it small is essential to generate results in this notebook in timely manner for demonstration purpose. For real use case, it can be increased to 1000 or 10000 to get most optimal choice of hyperparameters from the hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayesian Optimization technique implemented in gp_minimize method of Scikit Optimize package to choose optimal hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed computation time: 177.782 mins\n",
      "Best score: 0.559318001771511\n"
     ]
    }
   ],
   "source": [
    "def objective_dtc(params):\n",
    "    clf = DecisionTreeClassifier(\n",
    "        max_depth = params[0],\n",
    "        min_samples_leaf = params[1],\n",
    "        max_features = params[2],\n",
    "        min_samples_split = params[3],\n",
    "        random_state = 42\n",
    "    )\n",
    "    score = my_cross_val_score(clf, X=X_train, y=y_train)\n",
    "    return -score\n",
    "start_time = time.time()\n",
    "dtc_opt_result = gp_minimize(\n",
    "    func=objective_dtc,\n",
    "    dimensions=[\n",
    "        (5, 30),\n",
    "        (1, 10),\n",
    "        (0.1, 1.0),\n",
    "        (2, 20)\n",
    "    ],\n",
    "    random_state=42\n",
    ")\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "best_params_list_dtc = dtc_opt_result.x\n",
    "best_score_dtc = -dtc_opt_result.fun\n",
    "best_params_dtc = {\n",
    "    \"max_depth\": best_params_list_dtc[0],\n",
    "    \"min_samples_leaf\": best_params_list_dtc[1],\n",
    "    \"max_features\": best_params_list_dtc[2],\n",
    "    \"min_samples_split\": best_params_list_dtc[3]\n",
    "}\n",
    "print (\"Best score: \" + str(best_score_dtc))\n",
    "dtc = DecisionTreeClassifier()\n",
    "best_params_dtc[\"random_state\"] = 42\n",
    "dtc.set_params(**best_params_dtc)\n",
    "dtc.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]))\n",
    "pickle.dump(dtc, open('dtc_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dtc_cv = MyGridSearchCV(estimator=DecisionTreeClassifier(), cv=3, param_grid={\n",
    "#     'max_depth': [10, 15, 20],\n",
    "#     'min_samples_leaf': [3, 5, 7],\n",
    "#     'max_features': [None],\n",
    "#     'random_state': [42]\n",
    "# })\n",
    "\n",
    "# dtc_cv.fit(X=X_train, y=y_train)\n",
    "# dtc_cv.print_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(dtc_cv.best_estimator, open('dtc_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': None,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 50,\n",
       " 'random_state': None}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada_cv = MyGridSearchCV(param_grid={\n",
    "#     'base_estimator__min_samples_leaf': [3, 5],\n",
    "#     'base_estimator__max_depth': [3, 4, 5, 6, 7],\n",
    "#     'random_state': [42],\n",
    "#     'n_estimators': [10, 25, 50, 100]\n",
    "# }, estimator=clf, cv=3)\n",
    "# start_time = time.time()\n",
    "# ada_cv_result = ada_cv.fit(X_train, y_train)\n",
    "# elapsed_time = (time.time() - start_time) / 60\n",
    "# print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada_cv.print_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-031d650cfb3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     ],\n\u001b[1;32m     23\u001b[0m     \u001b[0mn_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-031d650cfb3a>\u001b[0m in \u001b[0;36mobjective_ada\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      9\u001b[0m     })\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_cross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-b20bd82a81e8>\u001b[0m in \u001b[0;36mmy_cross_val_score\u001b[0;34m(estimator, X, y)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"listing_id in @l_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"listing_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"listing_id in @l_test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"listing_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mf1_score_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 random_state)\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# Early termination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \"\"\"\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SAMME.R'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def objective_ada(params):\n",
    "#     clf = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "#     clf.set_params(** {\n",
    "#         \"base_estimator__min_samples_leaf\" : params[0],\n",
    "#         \"base_estimator__min_samples_split\" : params[1],\n",
    "#         \"base_estimator__max_depth\" : params[2],\n",
    "#         \"n_estimators\": params[3],\n",
    "#         \"random_state\": 42\n",
    "#     })\n",
    "    \n",
    "#     score = my_cross_val_score(clf, X=X_train, y=y_train)\n",
    "#     return -score\n",
    "\n",
    "# start_time = time.time()\n",
    "# ada_opt_result = gp_minimize(\n",
    "#     func=objective_ada,\n",
    "#     dimensions=[\n",
    "#         (1, 5),\n",
    "#         (2, 20),\n",
    "#         (5, 30),\n",
    "#         (10, 100)\n",
    "#     ],\n",
    "#     n_points=10,\n",
    "#     random_state=42\n",
    "# )\n",
    "# elapsed_time = (time.time() - start_time) / 60\n",
    "# print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "\n",
    "# best_params_list_ada = ada_opt_result.x\n",
    "# best_score_ada = -ada_opt_result.fun\n",
    "# print (\"Best score: \" + str(best_score_ada))\n",
    "# best_params_ada = {\n",
    "#     \"base_estimator__min_samples_leaf\": best_params_list_ada[0],\n",
    "#     \"base_estimator__min_samples_split\": best_params_list_ada[1],\n",
    "#     \"base_estimator__max_depth\": best_params_list_ada[2],\n",
    "#     \"n_estimators\": best_params_list_ada[3]\n",
    "# }\n",
    "# print(best_params_ada)\n",
    "# best_params_ada[\"random_state\"] = 42\n",
    "# ada = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "# ada.set_params(**best_params_ada)\n",
    "# ada.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]))\n",
    "# pickle.dump(ada, open('ada_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_estimator__max_depth': 20, 'base_estimator__min_samples_leaf': 5, 'base_estimator__min_samples_split': 18, 'n_estimators': 20}\n",
      "score: 0.562784356574439                             \n",
      "  1%|          | 1/100 [22:59<37:55:53, 1379.33s/it, best loss: -0.562784356574439]"
     ]
    }
   ],
   "source": [
    "def objective_ada(params):\n",
    "    clf = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "    clf.set_params(**params)    \n",
    "    score = my_cross_val_score(clf, X=X_train, y=y_train)\n",
    "    print(params)\n",
    "    print(\"score: \" + str(score))\n",
    "    return -score\n",
    "ada_param_space = {\n",
    "    \"base_estimator__min_samples_leaf\": hp.choice(\"base_estimator_min_samples_leaf\", range(1,6,2)),\n",
    "    \"base_estimator__min_samples_split\": hp.choice(\"base_estimator_min_samples_split\", [2*x for x in range(1,11,2)]),\n",
    "    \"base_estimator__max_depth\": hp.choice(\"base_estimator_max_depth\", range(5, 31, 5)),\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(10, 101, 10))\n",
    "}\n",
    "\n",
    "best_params_ada = fmin(objective_ada, ada_param_space, algo=tpe.suggest, max_evals=100)\n",
    "print (\"Best params: \")\n",
    "print (best_params_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using custom Grid Search Cross validation method implemented by me to choose optimal hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_leaf_nodes': [8, 16, 32, 64],\n",
    "              'max_depth': [5, 10],\n",
    "              'random_state': [42],\n",
    "              'n_estimators': [10, 25, 50]\n",
    "            }\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "gb_cv = MyGridSearchCV(estimator=clf, param_grid=param_grid, cv=3)\n",
    "gb_result = gb_cv.fit(X_train, y_train)\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_cv.print_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gb_cv.best_estimator, open('gb_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_gb(params):\n",
    "#     clf = GradientBoostingClassifier(            \n",
    "#             max_depth = params[0],\n",
    "#             n_estimators = params[1],\n",
    "#             random_state = 42\n",
    "#         )\n",
    "#     score = my_cross_val_score(clf, X=X_train, y=y_train)\n",
    "#     return -score\n",
    "\n",
    "# start_time = time.time()\n",
    "# gb_opt_result = gp_minimize(\n",
    "#     func=objective_gb,\n",
    "#     dimensions=[\n",
    "#         (5, 10),\n",
    "#         (10, 100)\n",
    "#     ],\n",
    "#     n_points=10,\n",
    "#     random_state=42\n",
    "# )\n",
    "# elapsed_time = (time.time() - start_time) / 60\n",
    "# print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "\n",
    "# best_params_list_gb = gb_opt_result.x\n",
    "# best_score_gb = -gb_opt_result.fun\n",
    "# best_params_gb = {\n",
    "#     \"max_depth\": best_params_list_gb[0],\n",
    "#     \"n_estimators\": best_params_list_gb[1]\n",
    "# }\n",
    "# print (\"Best score: \" + str(best_score_gb))\n",
    "# print(best_params_gb)\n",
    "\n",
    "# gb = GradientBoostingClassifier()\n",
    "# best_params_gb[\"random_state\"] = 42\n",
    "# gb.set_params(**best_params_gb)\n",
    "# gb.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]))\n",
    "# pickle.dump(gb, open('gb_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier()\n",
    "# clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#               'max_depth': [5, 10, 15, 20, None],\n",
    "#               'random_state': [42],\n",
    "#                 'min_samples_leaf': [1, 3, 5],\n",
    "#                 'n_estimators': [50]\n",
    "#             }\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# rf_cv = MyGridSearchCV(estimator=clf, param_grid=param_grid, cv=3)\n",
    "# rf_result = rf_cv.fit(X_train, y_train)\n",
    "\n",
    "# elapsed_time = (time.time() - start_time) / 60\n",
    "# print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_cv.print_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(rf_cv.best_estimator, open('rf_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(params):\n",
    "    clf = RandomForestClassifier(\n",
    "            min_samples_leaf = params[0],\n",
    "            max_depth = params[1],\n",
    "            n_estimators = params[2],\n",
    "            random_state=42\n",
    "        )\n",
    "    score = my_cross_val_score(clf, X=X_train, y=y_train)\n",
    "    return -score\n",
    "\n",
    "start_time = time.time()\n",
    "rf_opt_result = gp_minimize(\n",
    "    func=objective_gb,\n",
    "    dimensions=[\n",
    "        (1, 9),\n",
    "        (5, 30),\n",
    "        (10, 100)\n",
    "    ],\n",
    "    n_points=10,\n",
    "    random_state=42\n",
    ")\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "\n",
    "best_params_list_rf = rf_opt_result.x\n",
    "best_score_rf = -rf_opt_result.fun\n",
    "print (\"Best score: \" + str(best_score_rf))\n",
    "best_params_rf = {\n",
    "    \"min_samples_leaf\": best_params_list_rf[0],\n",
    "    \"max_depth\": best_params_list_rf[1],\n",
    "    \"n_estimators\": best_params_list_rf[2]\n",
    "}\n",
    "print(best_params_rf)\n",
    "\n",
    "rf = GradientBoostingClassifier()\n",
    "best_params_rf[\"random_state\"] = 42\n",
    "rf.set_params(**best_params_rf)\n",
    "rf.fit(X=X_train.drop(columns=[\"listing_id\"]), y=y_train.drop(columns=[\"listing_id\"]))\n",
    "pickle.dump(gb, open('rf_best.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction & Model Evaluation of Ensemble Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": \"rf_best.pkl\",\n",
    "    \"Gradient Boost\": \"gb_best.pkl\",\n",
    "    \"Ada Boost\": \"ada_best.pkl\",\n",
    "    \"Single Decision Tree\": \"dtc_best.pkl\"\n",
    "}\n",
    "for model_name in models.keys():\n",
    "    model_file_name = models[model_name]\n",
    "    model = pickle.load(open(model_file_name, 'rb'))\n",
    "    y_preds = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_preds, average=\"macro\")\n",
    "    print (\"Model: \" + model_name + \" f1-score: \" + str(f1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(n_features, learn_rate):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(150, input_shape=(n_features,),\n",
    "              kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(75, kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Dense(25, kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.10))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=learn_rate),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[\"available\"].value_counts()/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the data available for classification is somewhat imbalanced. I am applying various approaches to ensure balance in the training received by the neural network based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
